#!/usr/bin/env python3
"""
Reinforcement Learning (DQN) training script for TurtleBot3 in the kitchen world.
This version resets the simulation at the beginning of each episode by calling the
ROS2 service: /reset_simulation std_srvs/srv/Empty "{}".
 
Before running this script, launch your kitchen simulation:
    export TURTLEBOT3_MODEL=waffle_pi
    ros2 launch turtlebot3_gazebo turtlebot3_kitchen.launch.py
"""

import gym
from gym import spaces
import numpy as np
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from nav_msgs.msg import Odometry
from geometry_msgs.msg import Twist
from std_srvs.srv import Empty
import cv2
from cv_bridge import CvBridge
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import time
import threading
from rclpy.executors import SingleThreadedExecutor
from gym.wrappers import TimeLimit

# export TURTLEBOT3_MODEL=waffle_pi
# ros2 launch turtlebot3_gazebo turtlebot3_house.launch.py x_pose:=3 y_pose:=3

# ---------------------- ROS2 Node for Sensor Data ---------------------- #
class TurtleBotROS(Node):
    def __init__(self):
        super().__init__('turtlebot_kitchen_rl_node')
        self.image_sub = self.create_subscription(Image, '/camera/image_raw', self.image_callback, 10)
        self.odom_sub = self.create_subscription(Odometry, '/odom', self.odom_callback, 10)
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.bridge = CvBridge()
        self.latest_image = None
        self.latest_pose = None

    def image_callback(self, msg):
        try:
            # Convert the ROS image message to an OpenCV image (BGR8)
            self.latest_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        except Exception as e:
            self.get_logger().error(f"Image conversion failed: {e}")

    def odom_callback(self, msg):
        self.latest_pose = msg.pose.pose

# ---------------------- Gym Environment for Kitchen World ---------------------- #
class TurtleBotKitchenEnv(gym.Env):
    """
    Custom Gym environment for the TurtleBot3 simulator running in a kitchen world.
    
    Observations:
      - 'image': 84x84 RGB image from the robot's camera.
      - 'coords': A 4-element vector [current_x, current_y, target_x, target_y].
    
    Actions:
      - Discrete actions mapped to pre-defined (linear, angular) velocity commands.
    
    Each call to reset() will now reset the simulation via a ROS2 service call.
    """
    def __init__(self, target_pose=[3.0, 2.0]):
        super(TurtleBotKitchenEnv, self).__init__()

        # Initialize ROS2 if not already running.
        if not rclpy.ok():
            rclpy.init(args=None)
        self.ros_node = TurtleBotROS()
        # Start the ROS node spinning in a separate thread.
        self.thread = threading.Thread(target=rclpy.spin, args=(self.ros_node,), daemon=True)
        self.thread.start()
        self.step_count = 0
        self.max_step_count = 150

   

        # Define discrete action space.
        # Mapping: 0: stop; 1: forward; 2: turn left; 3: turn right;
        # 4: forward+left; 5: forward+right.
        self.action_map = {
            0: (0.0, 0.0),
            1: (0.2, 0.0),
            2: (0.0, 0.5),
            3: (0.0, -0.5),
            4: (0.2, 0.5),
            5: (0.2, -0.5)
        }
        self.action_space = spaces.Discrete(len(self.action_map))

        # Define observation space.
        self.observation_space = spaces.Dict({
            'image': spaces.Box(low=0, high=255, shape=(84, 84, 3), dtype=np.uint8),
            'coords': spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)
        })

        self.target_pose = target_pose

    def _get_obs(self):
        # Ensure that both image and odometry are available.
        while self.ros_node.latest_image is None or self.ros_node.latest_pose is None:
            time.sleep(0.1)
        # Process the image to 84x84.
        image = cv2.resize(self.ros_node.latest_image, (84, 84))
        # Extract current coordinates from odometry.
        cx = self.ros_node.latest_pose.position.x
        cy = self.ros_node.latest_pose.position.y
        gx, gy = self.target_pose
        coords = np.array([cx, cy, gx, gy], dtype=np.float32)
        return {'image': image, 'coords': coords}

    def reset(self):
        # Create a short‐lived node just for the reset service call
        self.step_count = 0
        reset_node = rclpy.create_node('reset_client_node')
        client     = reset_node.create_client(Empty, '/reset_simulation')

        if not client.wait_for_service(timeout_sec=5.0):
            reset_node.get_logger().error("Reset service unavailable!")
        else:
            # Build and spin a dedicated executor for this call
            executor = SingleThreadedExecutor()
            executor.add_node(reset_node)

            req    = Empty.Request()
            future = client.call_async(req)
            executor.spin_until_future_complete(future, timeout_sec=5.0)
            executor.shutdown()

            if future.result() is not None:
                reset_node.get_logger().info("Simulation reset!")
            else:
                reset_node.get_logger().error("Reset failed!")

        # Clean up the temporary node
        reset_node.destroy_node()

        # Give Gazebo time to restart and sensors to republish
        time.sleep(2.0)
        return self._get_obs()


    def step(self, action):
        linear, angular = self.action_map[action]
        twist = Twist()
        twist.linear.x = linear
        twist.angular.z = angular
        self.ros_node.cmd_vel_pub.publish(twist)

        # Allow time for the action to take effect.
        time.sleep(0.5)
        next_obs = self._get_obs()

        # Compute reward as negative Euclidean distance to the target.
        cx, cy, tx, ty = next_obs['coords']
        distance = np.sqrt((cx - tx)**2 + (cy - ty)**2)
        reward = -distance
        done = distance < 0.5  # Episode done if within 0.5 meters.

        self.step_count += 1
        if done == False:
            done = (self.step_count >= self.max_step_count)

        return next_obs, reward, done, {}

    def close(self):
        self.ros_node.destroy_node()
        rclpy.shutdown()

# ---------------------- DQN Network Definition ---------------------- #
class DQN(nn.Module):
    def __init__(self, n_actions):
        super(DQN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Flatten()
        )
        conv_out_size = 32 * 9 * 9  # adjust if you change the convs

        # coordinate encoder (4 inputs: cur_x, cur_y, goal_x, goal_y)
        self.fc_coords = nn.Sequential(
            nn.Linear(4, 64),
            nn.ReLU()
        )

        # combine and output Q‐values
        self.fc = nn.Sequential(
            nn.Linear(conv_out_size + 64, 256),
            nn.ReLU(),
            nn.Linear(256, n_actions)
        )

    def forward(self, image, coords):
        # Convert image shape from [batch, 84,84,3] to [batch, 3,84,84] and normalize.
        x = image.permute(0, 3, 1, 2).float() / 255.0
        x = self.conv(x)
        y = self.fc_coords(coords)
        x = torch.cat([x, y], dim=1)
        return self.fc(x)

# ---------------------- Replay Buffer ---------------------- #
class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.array, zip(*batch))
        return state, action, reward, next_state, done

    def __len__(self):
        return len(self.buffer)

# ---------------------- Utility Function ---------------------- #
def preprocess_obs(obs):
    """
    Converts an observation dictionary to PyTorch tensors.
    Returns:
        image: tensor of shape [1, 84, 84, 3]
        coords: tensor of shape [1, 4]
    """
    image = torch.tensor(obs['image'], dtype=torch.uint8).unsqueeze(0)
    coords = torch.tensor(obs['coords'], dtype=torch.float32).unsqueeze(0)
    return image, coords

# ---------------------- Training Loop ---------------------- #
def train():

    target_list = [
        [6.5, -1],
    ]

    env = TurtleBotKitchenEnv(target_pose=[6.5, -1])
    n_actions = env.action_space.n
    policy_net = DQN(n_actions)
    target_net = DQN(n_actions)
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)
    replay_buffer = ReplayBuffer(capacity=10000)

    num_episodes = 200
    batch_size = 32
    gamma = 0.99
    epsilon_start = 1.0
    epsilon_final = 0.1
    epsilon_decay = 500
    steps_done = 0

    def select_action(obs, epsilon):
        nonlocal steps_done
        steps_done += 1
        if random.random() < epsilon:
            return random.randrange(n_actions)
        else:
            with torch.no_grad():
                image, coords = preprocess_obs(obs)
                q_values = policy_net(image, coords)
                return q_values.max(1)[1].item()

    for i_episode in range(num_episodes):
        # Reset the simulation (calls the reset service) at the beginning of each episode.
        obs = env.reset()
        env.target_pose = random.choice(target_list)
        episode_reward = 0
        done = False
        while not done:
            epsilon = epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * steps_done / epsilon_decay)
            action = select_action(obs, epsilon)
            
             
            next_obs, reward, done, _ = env.step(action)
            
            replay_buffer.push(obs, action, reward, next_obs, done)
            obs = next_obs
            episode_reward = reward

            if len(replay_buffer) > batch_size:
                state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(batch_size)

                images = []
                coords = []
                next_images = []
                next_coords = []
                for state in state_batch:
                    img, crd = preprocess_obs(state)
                    images.append(img)
                    coords.append(crd)
                for state in next_state_batch:
                    img, crd = preprocess_obs(state)
                    next_images.append(img)
                    next_coords.append(crd)
                images = torch.cat(images, dim=0)
                coords = torch.cat(coords, dim=0)
                next_images = torch.cat(next_images, dim=0)
                next_coords = torch.cat(next_coords, dim=0)
                action_batch = torch.tensor(action_batch, dtype=torch.long)
                reward_batch = torch.tensor(reward_batch, dtype=torch.float32)
                done_batch = torch.tensor(done_batch, dtype=torch.float32)

                current_q = policy_net(images, coords).gather(1, action_batch.unsqueeze(1)).squeeze(1)
                with torch.no_grad():
                    max_next_q = target_net(next_images, next_coords).max(1)[0]
                expected_q = reward_batch + gamma * max_next_q * (1 - done_batch)
                loss = nn.MSELoss()(current_q, expected_q)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        if i_episode % 10 == 0:
            target_net.load_state_dict(policy_net.state_dict())

        print(f"Episode {i_episode} Reward: {episode_reward:.2f} with Target: {env.target_pose}")
        torch.save(
        policy_net.state_dict(),
        'policy.pth'
        )

    env.close()

if __name__ == '__main__':
    train()
